# Deploying, monitoring, and migrating AMQS Streams

[quote, AMQ Streams Product docs]
AMQ Streams simplifies the process of running Apache Kafka in an OpenShift cluster.


The purpose of this article is to discuss a lessons learned about a migration project with a client; we helped a client migrate an AMQ Streams cluster
from Openshift 3.11 to Openshift 4.7; along with that we also migrated over 100s kafka producers and consumers applications to the new cluster.


## Objectives

In condensed terms, there was a need to migrate a pre-existing AMQ Streams(v1.4) cluster from Openshift 3.11 to AMQ Streams(v1.7) on Openshift 4.7.

These are the primary goals:

* minimal downtime
* minimal to no data loss
* no message duplication
* new cluster should be able to handle more throughput that previous cluster
* monitor and alert on certain kafka cluster and applications(producers, consumers) states and behaviors.

## Migration Strategy

To successfully carry out this migration, we adopted the _Think-Do-Check_ mindset; in other words we used the concepts of _Dry-Run_ to come up with repeatable and predictable migration strategy.

This strategy involved the following points:

. make sure source and target clusters are monitored and all metrics needed to track mirroring progress are displayed on easy to access dashboards;
** Prometheus and Grafana was used in this case;
** we used the *Strimzi Kafka Exporter* dashboard to gauge consumer group lags, Offsets, Incoming and Outgoing Messages per second.
** we also used message count per topic to make a guesstimates on topic/partition size;
. Validate target cluster TTL configs
* Validate data retention periods match for source and target clusters
** `log.retention.hours` for time based TTL, 
** `log.retention.bytes` for byte size based retention
** this is important because it helps with making sure data has the same TTL across the two clusters.
. Validate all `KafkaTopic` resources have been created on target cluster with similar configs as the source cluster
. confirm all required `KafkaUser` resources with their respective RBAC privileges are created
* in this context, we had one `KafkaUser` resource per app
. deploy all apps on the target cluster and validate they spin up and down with no issues;
* once apps are healthy, shut them down until ready for migration;
* this action created all required consumer groups; additionally it allowed the apps to have _CURRENT_OFFSET_ of zero, which ultimately means the next time an app spins up, it will start reading at that *offset* rather than the latest *offset*; hence preventing apps from skipping messages when they are launched during cut over.
. deploy MirrorMaker2 on target cluster and begin the replication
* depending on data size, data ingestion rate, replication speed, it might take from minutes, days, to weeks before MM2 is caught up with source cluster.
** use the monitoring dashboards(source and target), kcat, and *kafka cli programs* for validation.

## Cut Over Plan

The cut over plan describes steps involved in switching over to the new cluster.

IMPORTANT: MirrorMaker2 must be running ahead of cut over time, this can be days or weeks. use data size, data ingestion rate and replication speed to estimate for how long you need MM2 running.

. Stop MM2 instance
* this action is performed after confirming target cluster has caught up with source cluster.
* use monitoring, kcat, kafka cli tools to help with this.
** *hint:* grab last n(100, 5000,...) messages from each cluster, encode each message to base64, write the output to files and diff the files.
** in our case we could not compare offsets because for source and target cluster, offsets were always different regardless of mirroring progress.
*** I hope this gets resolved in a near future.
. Turn off data ingestion into legacy cluster and point it to target cluster.
* this may be `KafkaConnect`, `Debezium for Change Data Capture`, or apps that pull data from some place and pump it into the kafka cluster.
. Spin down producer apps and wait for consumer apps to drain their respective topics.
** `producer apps` might behave as both producers and consumers, you need to be very diligent in this case;
** I would also recommend that you partition your applications into tiers of upstream, dam, and downstream based on data flow and dependency chain.
. once all topics are drained, spin down consumer apps.
* a topic is *drained* when consumer group _CURRENT-OFFSET_(read position) matches _LOG-END-OFFSET_(write position) or when _LAG_ is zero.
* this ensures all messages on source cluster have been processed.
. Turn on producers/upstream apps on the target cluster
. Spin up dam apps if any
. Spin up downstream apps.
. Validate cut over process is successful
* Look at data ingestion volume; should be nearly or exactly the same;
* If you apps are writing to data stores, validate you're getting similar ingestion or retrieval traffic patterns.
* Validate apps logs and that there are no errors or at least abnormal messages appearing in the logs.
* Watch for resource consumption spikes and network traffic patterns in the cluster

## Rollback Plan

This plan might be per use case basis, but the distilled form is as follow:

. Switch data ingestion valve to source cluster
. On target cluster
* spin down producer/upstream apps on target cluster
* let consumer apps drain their respective topics and shut them down.
. On source cluster
* turn on producer/upstream apps
* turn on consumer apps.

## Summary

In this article we have explained the strategies and plans involved in migration an AMQ Streams cluster in _Active/Passive_ mode between two Openshift deployments.

== Technical Implementation
include::technical-implementation.adoc[Technical Implementation]




