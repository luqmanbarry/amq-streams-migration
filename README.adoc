# Deploying, monitoring, and migrating AMQ Streams

[quote, AMQ Streams Product docs]
AMQ Streams simplifies the process of running Apache Kafka in an OpenShift cluster.


The purpose of this article is to discuss lessons learned from a migration project with a client; we helped them migrate an AMQ Streams cluster
from Openshift 3.11 to 4.7; along with that we also migrated over 100s Java event-driven microservices to the new cluster.


## Objectives

In condensed terms, there was a need to migrate a pre-existing AMQ Streams(v1.4) cluster from OCP 3.11 to AMQ Streams(v1.7) on OCP 4.7.

These are the primary goals:

* minimal downtime
* minimal to no data loss
* no message duplication
* new cluster should be able to handle more throughput that previous cluster
* monitor and alert on certain kafka cluster and applications(producers, consumers) states and behaviors.

## Migration Strategy

To successfully carry out this migration, we adopted the _Think-Do-Check_ mindset; in other words we used the concepts of _Dry-Run_ to come up with repeatable and predictable migration strategy.

This strategy involved the following points:

. make sure source and target clusters are monitored and all metrics needed to track mirroring progress are displayed on easy to access dashboards;
** Prometheus and Grafana was used in this case;
** we used the *Strimzi Kafka Exporter* dashboard to gauge consumer group lags, Offsets, Incoming and Outgoing Messages per second.
** we also used message count per topic to make a guesstimates on topic/partition size;
. Validate target cluster TTL configs
* Validate data retention periods match for source and target clusters
** `log.retention.hours` for time based TTL, 
** `log.retention.bytes` for byte size based retention
** this is important because it helps with making sure data has the same TTL across the two clusters.
. Validate all `KafkaTopic` resources have been created on target cluster with similar configs as the source cluster
. confirm all required `KafkaUser` resources with their respective RBAC privileges are created
* in this context, we had one `KafkaUser` resource per app
. deploy all apps on the target cluster and validate they spin up and down with no issues;
* once apps are healthy, shut them down until ready for migration;
* this action created all required consumer groups; additionally it allowed the apps to have _CURRENT_OFFSET_ of zero, which ultimately means the next time an app spins up, it will start reading at that *offset* rather than the latest *offset*; hence preventing apps from skipping messages when they are launched during cut over.
. deploy MirrorMaker2 on target cluster and begin the replication
* depending on data size, data ingestion rate, replication speed, it might take from minutes, days, to weeks before MM2 is caught up with source cluster.
** use the monitoring dashboards(source and target), kcat, and *kafka cli programs* for validation.

## Cut Over Plan

The cut over plan describes steps involved in switching over to the new cluster.

IMPORTANT: MirrorMaker2 must be running ahead of cut over time, this can be days or weeks. use data size, data ingestion rate and replication speed to estimate for how long you need MM2 running.

. Stop MM2 instance
* this action is performed after confirming target cluster has caught up with source cluster.
* use monitoring, kcat, kafka cli tools to help with this.
** *hint:* grab last n(100, 5000,...) messages from each cluster, encode each message to base64, write the output to files and diff the files.
** in our case we could not compare offsets because for source and target cluster, offsets were always different regardless of mirroring progress.
*** I hope this gets resolved in a near future.
. Turn off data ingestion into legacy cluster and point it to target cluster.
* this may be `KafkaConnect`, `Debezium for Change Data Capture`, or apps that pull data from some place and pump it into the kafka cluster.
. Spin down producer apps and wait for consumer apps to drain their respective topics.
** `producer apps` might behave as both producers and consumers, you need to be very diligent in this case;
** I would also recommend that you partition your applications into tiers of upstream, dam, and downstream based on data flow and dependency chain.
. once all topics are drained, spin down consumer apps.
* a topic is *drained* when consumer group _CURRENT-OFFSET_(read position) matches _LOG-END-OFFSET_(write position) or when _LAG_ is zero.
* this ensures all messages on source cluster have been processed.
. Turn on producers/upstream apps on the target cluster
. Spin up dam apps if any
. Spin up downstream apps.
. Validate cut over process is successful
* Look at data ingestion volume; should be nearly or exactly the same;
* If your apps are writing to data stores, validate you're getting similar ingestion or read traffic patterns.
* Validate apps logs and that there are no errors or at least abnormal messages appearing in the logs.
* Watch for resource consumption spikes and network traffic patterns in the cluster.

## Rollback Plan

This plan might be per use case basis, but the distilled form is as follow:

. Switch data ingestion valve to source cluster
. On target cluster
* spin down producer/upstream apps on target cluster
* let consumer apps drain their respective topics and shut them down.
. On source cluster
* turn on producer/upstream apps
* turn on consumer apps.

## Issues faced and how we solved them

### 1. MirrorMaker2 replication strategy

In simple terms when mirroring an existing cluster:

* set `mirrors.sourceConnector.config.auto.offset.reset: latest`
** when replicating historical data is not of concerns
*** an example use case might be when MM2 is set up at the same time as the AMQ Streams cluster
*** or for some requirement, you just want to begin mirroring at the end of topic; no concerns for data loss.

* set `mirrors[].sourceConnector.config.auto.offset.reset: earliest`
** when replicating historical data is a requirement
** when you want all available historical data in the source cluster copied over to the target cluster
** an example use case might be when migrating an existing AMQ Streams cluster to another with _minimal or zero data loss_.
** or you want to setup and _active/passive_ setup whereby the passive cluster is installed at a moment when the active cluster have already seen data ingested.

* https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset[Red the Kafka docs for more]

### 2. Pods killed due to Out Of Memory in new cluster

Pods were being terminated in the target cluster, at first we thought it was the JVM eating up all the pod memory. We implemented JVM boundaries whereby we _-Xms_ and _Xmx_ values as arguments for the _ENTRYPOINT_ command of the container image; `java -jar` in this case. Despite this change we still got OOMKilled errors, pods were being killed.

* https://docs.openshift.com/container-platform/4.8/nodes/clusters/nodes-cluster-resource-configure.html[Click here for more on cluster memory configuration]

The next level of our troubleshooting, which by the way caused some head scratches for few days, is to increase the PID limit of the nodes in the RHOCP cluster. This issue was due to some of the apps creating more processes in their respective containers than the default PID limit set at *1024*; hence _OOMKilled_ error after the pod runs for few hours despite having an _HorizontalPodAutoscaler_ resource monitoring this particular application for scaling needs.

* https://kubernetes.io/docs/concepts/policy/pid-limiting/[Read on Kubernetes docs about PIDs limit]
* https://computingforgeeks.com/change-pids-limit-value-in-openshift/[Click here for how to change PIDs limit in Openshift]


## Technical Implementation

include::technical-implementation.adoc[]

## AMQ-Streams commands Cheat-Sheet

include::AMQ-Streams-command-cheat-sheet.pdf[]

## Summary

In this article we have explained the strategies and plans involved in migration an AMQ Streams cluster in _Active/Passive_ mode between two Openshift deployments.

Happy coding!




